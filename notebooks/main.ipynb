{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "os.chdir(\"/home/yw699/codes/LLM-halu\")\n",
    "sys.path.append(os.path.abspath(\"src\"))\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6,7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import math\n",
    "from dataset import Dataset\n",
    "from prompt_engineer import PromptGenerator\n",
    "from models import HuggingfaceModel\n",
    "from utils import *\n",
    "from metrics import *\n",
    "import logging\n",
    "import wandb\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import torch\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"configs/experiment_config1.yaml\", \"r\") as file:\n",
    "    config = yaml.safe_load(file)\n",
    "    \n",
    "wandb_config = config[\"wandb\"]\n",
    "metrics_config = config[\"metrics\"]\n",
    "experiment_details = {'config': config}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user = os.environ['USER']\n",
    "slurm_jobid = os.getenv('SLURM_JOB_ID', None)\n",
    "scratch_dir = os.getenv('SCRATCH_DIR', '.')\n",
    "entity = os.getenv('WANDB_SEM_UNC_ENTITY', None)\n",
    "\n",
    "dir = f\"{scratch_dir}/{user}/{entity}\"\n",
    "if not os.path.exists(dir):\n",
    "        os.makedirs(dir)\n",
    "project = config[\"wandb\"][\"project\"]\n",
    "\n",
    "if config[\"wandb\"][\"debug\"]:\n",
    "    project = f\"{project}_debug\"\n",
    "\n",
    "experiment_lot = config[\"wandb\"]['experiment_lot']\n",
    "notes=f'slurm_id: {slurm_jobid}, experiment_lot: {experiment_lot}'\n",
    "\n",
    "wandb.init(\n",
    "    entity=entity,\n",
    "    project= project,\n",
    "    dir=dir,\n",
    "    config=config,\n",
    "    notes=notes,\n",
    ")\n",
    "\n",
    "logging.info('Finished wandb init.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_loader = Dataset(config)\n",
    "train_dataset, validation_dataset = dataset_loader.load_data()\n",
    "\n",
    "\n",
    "if not isinstance(train_dataset, list):\n",
    "        logging.info('Train dataset: %s', train_dataset)\n",
    "\n",
    "answerable_indices, unanswerable_indices = split_dataset(train_dataset)\n",
    "\n",
    "\n",
    "if config[\"dataset\"]['answerable_only']:\n",
    "        unanswerable_indices = []\n",
    "        val_answerable, val_unanswerable = split_dataset(validation_dataset)\n",
    "        del val_unanswerable\n",
    "        validation_dataset = [validation_dataset[i] for i in val_answerable]\n",
    "        train_dataset = [train_dataset[i] for i in answerable_indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The prompt is used in every sampling process.\n",
    "promptgenerator = PromptGenerator(config,train_dataset)\n",
    "few_shot_prompt,prompt_indices = promptgenerator.construct_fewshot_prompt_by_nums(2)\n",
    "experiment_details['prompt_indices'] = prompt_indices\n",
    "experiment_details['prompt'] = few_shot_prompt\n",
    "experiment_details['BRIEF'] = promptgenerator.BRIEF\n",
    "logging.info('Prompt is: %s', few_shot_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface_model = HuggingfaceModel(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## P_True Measure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = get_metric('squad')\n",
    "validation_promptgenerator = PromptGenerator(config,validation_dataset)\n",
    "p_true_evaluator = PTrueEvaluator(config,huggingface_model,promptgenerator,validation_promptgenerator,metric,experiment_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_true_few_shot_prompt = p_true_evaluator.construct_few_shot_prompt_for_p_true(few_shot_prompt,5,3)\n",
    "#wandb.config.update({'p_true_num_fewshot': len_p_true}, allow_val_change=True)\n",
    "#wandb.log(dict(len_p_true=len_p_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_true_evaluator.all_evaluate(few_shot_prompt,1.0,2,p_true_few_shot_prompt,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'config': {'wandb': {'debug': False, 'project': 'test', 'experiment_lot': 'MyExperiment'}, 'dataset': {'name': 'squad', 'seed': 42, 'answerable_only': True}, 'prompt': {'few-shot': False, 'shot_num': 3, 'brief_always': True, 'use_context': True, 'add_tag': True, 'prompt_template_path': './data/prompt_templates/ask_templates/test2.txt'}, 'model': {'model_name': 'meta-llama/Llama-2-7b-hf', 'stop_sequences': 'default', 'max_new_tokens': 50}, 'sample': {'temperature': 1.0, 'sample_count': 5, 'sampling_method': 'simple_sample'}, 'metrics': [{'name': 'p_true', 'p_true_num_fewshot': 2}, {'name': 'accuracy'}, {'name': 'diversity'}], 'p_true': {'compute_p_true': True, 'get_training_set_generations': True, 'get_training_set_generations_most_likely_only': True, 'compute_accuracy_at_all_temps': True, 'p_true_hint': False}}, 'prompt_indices': [18303, 24501], 'prompt': 'Answer the following question as briefly as possible.\\nContext: Jean-Jacques Rousseau was the first of many to present the Alps as a place of allure and beauty, banishing the prevalent conception of the mountains as a hellish wasteland inhabited by demons. Rousseau\\'s conception of alpine purity was later emphasized with the publication of Albrecht von Haller\\'s poem Die Alpen that described the mountains as an area of mythical purity. Late in the 18th century the first wave of Romantics such as Goethe and Turner came to admire the scenery; Wordsworth visited the area in 1790, writing of his experiences in The Prelude. Schiller later wrote the play William Tell romanticising Swiss independence. After the end of the Napoleonic Wars, the Alpine countries began to see an influx of poets, artists, and musicians, as visitors came to experience the sublime effects of monumental nature.\\nQuestion: Who was the first of many to present the Alps as a place of allure and beauty?\\nAnswer: Jean-Jacques Rousseau\\n\\nAnswer the following question as briefly as possible.\\nContext: Grape juice is obtained from crushing and blending grapes into a liquid. The juice is often sold in stores or fermented and made into wine, brandy, or vinegar. Grape juice that has been pasteurized, removing any naturally occurring yeast, will not ferment if kept sterile, and thus contains no alcohol. In the wine industry, grape juice that contains 7–23% of pulp, skins, stems and seeds is often referred to as \"must\". In North America, the most common grape juice is purple and made from Concord grapes, while white grape juice is commonly made from Niagara grapes, both of which are varieties of native American grapes, a different species from European wine grapes. In California, Sultana (known there as Thompson Seedless) grapes are sometimes diverted from the raisin or table market to produce white juice.\\nQuestion: What juice is made when grapes are crushed and blended?\\nAnswer: Grape juice\\n\\n', 'BRIEF': 'Answer the following question as briefly as possible.\\n', 'p_true_indices': [43968, 26053, 17832], 'p_true_responses': {43968: {'responses': ['Interframe compression', 'Interframe compression', 'Interframe compression', 'Interframe compression', 'Interframe compression', 'Interframe compression'], 'most_likely_response': 'Interframe compression', 'is_correct': 1.0}, 26053: {'responses': ['2007', 'In 2007', '2007', '2007', '2007', 'In 2007'], 'most_likely_response': '2007', 'is_correct': 1.0}, 17832: {'responses': ['John I Albert', 'John I Albert', 'John I Albert', 'John I Albert', 'John I Albert', 'John I Albert'], 'most_likely_response': 'John I Albert', 'is_correct': 1.0}}, 'p_true_few_shot_prompt': 'Question: What copies data from one frame to another?\\nBrainstormed Answers: Interframe compression \\nInterframe compression \\nInterframe compression \\nInterframe compression \\nInterframe compression \\nInterframe compression \\nPossible answer: Interframe compression\\nIs the possible answer:\\nA) True\\nB) False\\nThe possible answer is: A\\nQuestion: When did two whisleblowers allege that Boutris  attempted to ground Southwest Airlines?\\nBrainstormed Answers: 2007 \\nIn 2007 \\n2007 \\n2007 \\n2007 \\nIn 2007 \\nPossible answer: 2007\\nIs the possible answer:\\nA) True\\nB) False\\nThe possible answer is: A\\nQuestion: Who won the election of 1492?\\nBrainstormed Answers: John I Albert \\nJohn I Albert \\nJohn I Albert \\nJohn I Albert \\nJohn I Albert \\nJohn I Albert \\nPossible answer: John I Albert\\nIs the possible answer:\\nA) True\\nB) False\\nThe possible answer is: A', 'train': {'indices': [59117, 49425, 45839]}, 'validation': {'indices': [4824, 3554, 2120]}}\n"
     ]
    }
   ],
   "source": [
    "print(experiment_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropies = defaultdict(list)\n",
    "validation_embeddings, validation_is_true, validation_answerable = [], [], []\n",
    "p_trues = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P_ik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.compute_p_ik or args.compute_p_ik_answerable:\n",
    "    # Assemble training data for embedding classification.\n",
    "    train_is_true, train_embeddings, train_answerable = [], [], []\n",
    "    for tid in train_generations:\n",
    "        most_likely_answer = train_generations[tid]['most_likely_answer']\n",
    "        train_embeddings.append(most_likely_answer['embedding'])\n",
    "        train_is_true.append(most_likely_answer['accuracy'])\n",
    "        train_answerable.append(is_answerable(train_generations[tid]))\n",
    "    train_is_false = [0.0 if is_t else 1.0 for is_t in train_is_true]\n",
    "    train_unanswerable = [0.0 if is_t else 1.0 for is_t in train_answerable]\n",
    "    logging.info('Unanswerable prop on p_ik training: %f', np.mean(train_unanswerable))\n",
    "\n",
    "if args.compute_p_ik:\n",
    "    logging.info('Starting training p_ik on train embeddings.')\n",
    "    # Train classifier of correct/incorrect from embeddings.\n",
    "    p_ik_predictions = get_p_ik(\n",
    "        train_embeddings=train_embeddings, is_false=train_is_false,\n",
    "        eval_embeddings=validation_embeddings, eval_is_false=validation_is_false)\n",
    "    result_dict['uncertainty_measures']['p_ik'] = p_ik_predictions\n",
    "    logging.info('Finished training p_ik on train embeddings.')\n",
    "\n",
    "if args.compute_p_ik_answerable:\n",
    "    # Train classifier of answerable/unanswerable.\n",
    "    p_ik_predictions = get_p_ik(\n",
    "        train_embeddings=train_embeddings, is_false=train_unanswerable,\n",
    "        eval_embeddings=validation_embeddings, eval_is_false=validation_unanswerable)\n",
    "    result_dict['uncertainty_measures']['p_ik_unanswerable'] = p_ik_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entailment_model = EntailmentDeberta()\n",
    "\n",
    "\n",
    "# compute_context_entails_response\n",
    "entropies['context_entails_response'].append(context_entails_response(context, responses, entailment_model))\n",
    "\n",
    "# condition_on_question\n",
    "responses = [f'{question} {r}' for r in responses]\n",
    "\n",
    "\n",
    "if args.compute_predictive_entropy:\n",
    "    # Token log likelihoods. Shape = (n_sample, n_tokens)\n",
    "    if not args.use_all_generations:\n",
    "        log_liks = [r[1] for r in full_responses[:args.use_num_generations]]\n",
    "    else:\n",
    "        log_liks = [r[1] for r in full_responses]\n",
    "\n",
    "    for i in log_liks:\n",
    "        assert i\n",
    "\n",
    "    if args.compute_context_entails_response:\n",
    "        # Compute context entails answer baseline.\n",
    "        entropies['context_entails_response'].append(context_entails_response(context, responses, entailment_model))\n",
    "\n",
    "    if args.condition_on_question and args.entailment_model == 'deberta':\n",
    "        responses = [f'{question} {r}' for r in responses]\n",
    "\n",
    "    # Compute semantic ids.\n",
    "    semantic_ids = get_semantic_ids(\n",
    "        responses, model=entailment_model,\n",
    "        strict_entailment=args.strict_entailment, example=example)\n",
    "\n",
    "    result_dict['semantic_ids'].append(semantic_ids)\n",
    "\n",
    "    # Compute entropy from frequencies of cluster assignments.\n",
    "    entropies['cluster_assignment_entropy'].append(cluster_assignment_entropy(semantic_ids))\n",
    "\n",
    "    # Length normalization of generation probabilities.\n",
    "    log_liks_agg = [np.mean(log_lik) for log_lik in log_liks]\n",
    "\n",
    "    # Compute naive entropy.\n",
    "    entropies['regular_entropy'].append(predictive_entropy(log_liks_agg))\n",
    "\n",
    "    # Compute semantic entropy.\n",
    "    log_likelihood_per_semantic_id = logsumexp_by_id(semantic_ids, log_liks_agg, agg='sum_normalized')\n",
    "    pe = predictive_entropy_rao(log_likelihood_per_semantic_id)\n",
    "    entropies['semantic_entropy'].append(pe)\n",
    "\n",
    "    # pylint: disable=invalid-name\n",
    "    log_str = 'semantic_ids: %s, avg_token_log_likelihoods: %s, entropies: %s'\n",
    "    entropies_fmt = ', '.join([f'{i}:{j[-1]:.2f}' for i, j in entropies.items()])\n",
    "    # pylint: enable=invalid-name\n",
    "    logging.info(80*'#')\n",
    "    logging.info('NEW ITEM %d at id=`%s`.', idx, tid)\n",
    "    logging.info('Context:')\n",
    "    logging.info(example['context'])\n",
    "    logging.info('Question:')\n",
    "    logging.info(question)\n",
    "    logging.info('True Answers:')\n",
    "    logging.info(example['reference'])\n",
    "    logging.info('Low Temperature Generation:')\n",
    "    logging.info(most_likely_answer['response'])\n",
    "    logging.info('Low Temperature Generation Accuracy:')\n",
    "    logging.info(most_likely_answer['accuracy'])\n",
    "    logging.info('High Temp Generation:')\n",
    "    logging.info([r[0] for r in full_responses])\n",
    "    logging.info('High Temp Generation:')\n",
    "    logging.info(log_str, semantic_ids, log_liks_agg, entropies_fmt)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic_uncertainty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic_uncertainty2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输入results_old\n",
    "\n",
    "'''\n",
    "results_old['validation_is_false']\n",
    "results_old['uncertainty_measures']\n",
    "results_old['uncertainty_measures']['p_false']\n",
    "results_old['uncertainty_measures']['p_false_fixed']\n",
    "results_old['validation_unanswerable']\n",
    "\n",
    "\n",
    "#### for measure_name, measure_values in rum.items():\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for measure_name, measure_values in rum.items():\n",
    "        logging.info('Computing for uncertainty measure `%s`.', measure_name)\n",
    "\n",
    "        # Validation accuracy.\n",
    "        validation_is_falses = [\n",
    "            results_old['validation_is_false'],\n",
    "            results_old['validation_unanswerable']\n",
    "        ]\n",
    "\n",
    "        logging_names = ['', '_UNANSWERABLE']\n",
    "\n",
    "        # Iterate over predictions of 'falseness' or 'answerability'.\n",
    "        # 指标计算\n",
    "        for validation_is_false, logging_name in zip(validation_is_falses, logging_names):\n",
    "            name = measure_name + logging_name\n",
    "            result_dict['uncertainty'][name] = {}\n",
    "\n",
    "            validation_is_false = np.array(validation_is_false)\n",
    "            validation_accuracy = 1 - validation_is_false\n",
    "            if len(measure_values) > len(validation_is_false):\n",
    "                # This can happen, but only for p_false.\n",
    "                if 'p_false' not in measure_name:\n",
    "                    raise ValueError\n",
    "                logging.warning(\n",
    "                    'More measure values for %s than in validation_is_false. Len(measure values): %d, Len(validation_is_false): %d',\n",
    "                    measure_name, len(measure_values), len(validation_is_false))\n",
    "                measure_values = measure_values[:len(validation_is_false)]\n",
    "\n",
    "            # fargs = {\n",
    "            #     'AUROC': [validation_is_false, measure_values],\n",
    "            #     'area_under_thresholded_accuracy': [validation_accuracy, measure_values],\n",
    "            #     'mean_uncertainty': [measure_values]}\n",
    "\n",
    "            # for answer_fraction in answer_fractions:\n",
    "            #     fargs[f'accuracy_at_{answer_fraction}_answer_fraction'] = [validation_accuracy, measure_values]\n",
    "\n",
    "            # for fname, (function, bs_function) in eval_metrics.items():\n",
    "            #     metric_i = function(*fargs[fname])\n",
    "            #     result_dict['uncertainty'][name][fname] = {}\n",
    "            #     result_dict['uncertainty'][name][fname]['mean'] = metric_i\n",
    "            #     logging.info(\"%s for measure name `%s`: %f\", fname, name, metric_i)\n",
    "            #     result_dict['uncertainty'][name][fname]['bootstrap'] = bs_function(\n",
    "            #         function, rng)(*fargs[fname])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 将结果都存放在 result_dict 中\n",
    "result_dict = {'performance': {}, 'uncertainty': {}}\n",
    "\n",
    "## 测量 performance\n",
    "all_accuracies['accuracy'] = 1 - np.array(results_old['validation_is_false'])\n",
    "\n",
    "for name, target in all_accuracies.items():\n",
    "    result_dict['performance'][name] = {}\n",
    "    result_dict['performance'][name]['mean'] = np.mean(target)\n",
    "    result_dict['performance'][name]['bootstrap'] = bootstrap(np.mean, rng)(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 修改p_true\n",
    "##转换p_true为概率值\n",
    "rum = results_old['uncertainty_measures']\n",
    "if 'p_false' in rum and 'p_false_fixed' not in rum:\n",
    "    # Restore log probs true: y = 1 - x --> x = 1 - y.\n",
    "    # Convert to probs --> np.exp(1 - y).\n",
    "    # Convert to p_false --> 1 - np.exp(1 - y).\n",
    "    rum['p_false_fixed'] = [1 - np.exp(1 - x) for x in rum['p_false']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "val_metrics 是一个字典，键是指标名称（如 'AUROC'），值是一个元组 (function, bs_function)\n",
    "function 用于计算该指标的函数。\n",
    "bs_function：用于对该指标进行 bootstrap 分析 的函数\n",
    "\n",
    "\n",
    "\n",
    "eval_metrics = {\n",
    "    'AUROC': (auroc, compatible_bootstrap),\n",
    "    'area_under_thresholded_accuracy': (area_under_thresholded_accuracy, compatible_bootstrap),\n",
    "    'mean_uncertainty': (np.mean, bootstrap)\n",
    "    }\n",
    "\n",
    "'''\n",
    "\n",
    " eval_metrics = dict(zip(\n",
    "        ['AUROC', 'area_under_thresholded_accuracy', 'mean_uncertainty'],\n",
    "        list(zip([auroc, area_under_thresholded_accuracy, np.mean],[compatible_bootstrap, compatible_bootstrap, bootstrap])),\n",
    "    ))\n",
    "\n",
    "for answer_fraction in answer_fractions:\n",
    "        key = f'accuracy_at_{answer_fraction}_answer_fraction'\n",
    "        eval_metrics[key] = [functools.partial(accuracy_at_quantile, quantile=answer_fraction),compatible_bootstrap]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for measure_name, measure_values in rum.items():\n",
    "        logging.info('Computing for uncertainty measure `%s`.', measure_name)\n",
    "\n",
    "        # Validation accuracy.\n",
    "        validation_is_falses = [\n",
    "            results_old['validation_is_false'],\n",
    "            results_old['validation_unanswerable']\n",
    "        ]\n",
    "\n",
    "        logging_names = ['', '_UNANSWERABLE']\n",
    "\n",
    "        # Iterate over predictions of 'falseness' or 'answerability'.\n",
    "        # 指标计算\n",
    "        for validation_is_false, logging_name in zip(validation_is_falses, logging_names):\n",
    "            name = measure_name + logging_name\n",
    "            result_dict['uncertainty'][name] = {}\n",
    "\n",
    "            validation_is_false = np.array(validation_is_false)\n",
    "            validation_accuracy = 1 - validation_is_false\n",
    "            if len(measure_values) > len(validation_is_false):\n",
    "                # This can happen, but only for p_false.\n",
    "                if 'p_false' not in measure_name:\n",
    "                    raise ValueError\n",
    "                logging.warning(\n",
    "                    'More measure values for %s than in validation_is_false. Len(measure values): %d, Len(validation_is_false): %d',\n",
    "                    measure_name, len(measure_values), len(validation_is_false))\n",
    "                measure_values = measure_values[:len(validation_is_false)]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                # fargs 字典：评估任务 + 所需要的指标\n",
    "                # 实验指标通过fargs字典与eval_metrics字典实现可拓展\n",
    "                # 汇总每个评估指标以及所需要的不同输入数据\n",
    "                fargs = {\n",
    "                        'AUROC': [validation_is_false, measure_values],\n",
    "                        'area_under_thresholded_accuracy': [validation_accuracy, measure_values],\n",
    "                        'mean_uncertainty': [measure_values]}\n",
    "\n",
    "                #添加新的评估任务\n",
    "                for answer_fraction in answer_fractions:\n",
    "                    fargs[f'accuracy_at_{answer_fraction}_answer_fraction'] = [validation_accuracy, measure_values]\n",
    "\n",
    "                '''\n",
    "                fargs = {\n",
    "                    'AUROC': [validation_is_false, measure_values],\n",
    "                    'area_under_thresholded_accuracy': [validation_accuracy, measure_values],\n",
    "                    'mean_uncertainty': [measure_values],\n",
    "                    'accuracy_at_0.5_answer_fraction': [validation_accuracy, measure_values],\n",
    "                    'accuracy_at_0.75_answer_fraction': [validation_accuracy, measure_values]\n",
    "                }\n",
    "                '''\n",
    "\n",
    "\n",
    "                for fname, (function, bs_function) in eval_metrics.items():\n",
    "                    metric_i = function(*fargs[fname])\n",
    "                    result_dict['uncertainty'][name][fname] = {}\n",
    "                    result_dict['uncertainty'][name][fname]['mean'] = metric_i\n",
    "                    logging.info(\"%s for measure name `%s`: %f\", fname, name, metric_i)\n",
    "                    result_dict['uncertainty'][name][fname]['bootstrap'] = bs_function(\n",
    "                        function, rng)(*fargs[fname])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##最终的result_dict\n",
    "\n",
    "'''\n",
    "result_dict = {\n",
    "    'performance': {},  # 存储性能指标的结果\n",
    "    'uncertainty': {}   # 存储不确定性测量的结果\n",
    "}\n",
    "\n",
    "'performance': {\n",
    "    'accuracy': {  # 性能指标名称\n",
    "        'mean': 0.95,  # 准确率的均值\n",
    "        'bootstrap': [0.94, 0.96]  # 使用 bootstrap 计算的置信区间\n",
    "    }\n",
    "}\n",
    "\n",
    "'uncertainty': {\n",
    "    'p_false': {  # 不确定性测量名称（如 p_false）\n",
    "        'AUROC': {\n",
    "            'mean': 0.85,  # AUROC 的均值\n",
    "            'bootstrap': [0.83, 0.87]  # AUROC 的置信区间\n",
    "        },\n",
    "        'mean_uncertainty': {\n",
    "            'mean': 0.3,  # 不确定性测量的均值\n",
    "            'bootstrap': [0.28, 0.32]  # 不确定性测量的置信区间\n",
    "        },\n",
    "        'accuracy_at_0.8_answer_fraction': {\n",
    "            'mean': 0.88,  \n",
    "            'bootstrap': [0.86, 0.89]\n",
    "        },\n",
    "        ...\n",
    "    },\n",
    "    'p_false_UNANSWERABLE': {  # 对应另一个验证指标的结果\n",
    "        'AUROC': {\n",
    "            'mean': 0.75,\n",
    "            'bootstrap': [0.73, 0.77]\n",
    "        },\n",
    "        ...\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare: performance of this uncertain metrics\n",
    "Through the value of auroc, we compare these methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import wandb\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_metrics = {\n",
    "    \"uncertainty\": {\n",
    "        \"semantic_entropy\": {\n",
    "            \"AUROC\": {\"mean\": 0.75},\n",
    "            \"Accuracy\": {\"mean\": 0.85}\n",
    "        },\n",
    "        \"cluster_assignment_entropy\": {\n",
    "            \"AUROC\": {\"mean\": 0.72},\n",
    "            \"Accuracy\": {\"mean\": 0.83}\n",
    "        },\n",
    "        \"regular_entropy\": {\n",
    "            \"AUROC\": {\"mean\": 0.70},\n",
    "            \"Accuracy\": {\"mean\": 0.80}\n",
    "        },\n",
    "        \"p_false\": {\n",
    "            \"AUROC\": {\"mean\": 0.68},\n",
    "            \"Accuracy\": {\"mean\": 0.78}\n",
    "        },\n",
    "        \"p_ik\": {\n",
    "            \"AUROC\": {\"mean\": 0.74},\n",
    "            \"Accuracy\": {\"mean\": 0.82}\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb_id = 'h1scz5qz'\n",
    "# if wandb_id == 'YOUR_ID':\n",
    "#     raise ValueError('Need to provide wandb_id of demo run!')\n",
    "# def restore_file(wandb_id, filename='wandb-summary.json'):\n",
    "#     files_dir = 'notebooks/restored_files'    \n",
    "#     os.system(f'mkdir -p {files_dir}')\n",
    "\n",
    "#     api = wandb.Api()\n",
    "#     run = api.run(f'semantic_uncertainty/{wandb_id}')\n",
    "\n",
    "#     path = f'{files_dir}/{filename}'\n",
    "#     os.system(f'rm -rf {path}')\n",
    "#     run.file(filename).download(root=files_dir, replace=True, exist_ok=False)\n",
    "#     with open(path, 'r') as f:\n",
    "#         out = json.load(f)\n",
    "#     return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uncertainty_df(metrics):\n",
    "    data = []\n",
    "    for method in metrics['uncertainty']:\n",
    "        for metric in metrics['uncertainty'][method]:\n",
    "            mean = metrics['uncertainty'][method][metric]['mean']\n",
    "            data.append([method, metric, mean])\n",
    "    df = pd.DataFrame(data, columns=['method', 'metric', 'means'])\n",
    "    main_methods = ['semantic_entropy', 'cluster_assignment_entropy', 'regular_entropy', 'p_false', 'p_ik']\n",
    "    df = df.set_index('method').loc[main_methods].reset_index()\n",
    "    main_names = ['Semantic entropy', 'Discrete Semantic Entropy', 'Naive Entropy', 'p(True)', 'Embedding Regression']\n",
    "    conversion = dict(zip(main_methods, main_names))\n",
    "    df['method'] = df.method.map(lambda x: conversion[x])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unc_df = get_uncertainty_df(example_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = 'AUROC'\n",
    "unc_df.set_index('metric').loc[metric].plot.bar(x='method', y='means')\n",
    "plt.gca().set_ylabel(metric)\n",
    "plt.gca().grid(axis='y')\n",
    "plt.gca().set_ylim(0.6, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = 'Accuracy'\n",
    "unc_df.set_index('metric').loc[metric].plot.bar(x='method', y='means')\n",
    "plt.gca().set_ylabel(metric)\n",
    "plt.gca().grid(axis='y')\n",
    "plt.gca().set_ylim(0.6, 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "semantic_uncertainty",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
