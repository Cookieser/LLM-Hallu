{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "os.chdir(\"/home/yw699/codes/LLM-halu\")\n",
    "sys.path.append(os.path.abspath(\"src\"))\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6,7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import math\n",
    "from dataset import Dataset\n",
    "from prompt_engineer import PromptGenerator\n",
    "from models import HuggingfaceModel\n",
    "from utils import *\n",
    "from metrics import *\n",
    "import logging\n",
    "import wandb\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import torch\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"configs/experiment_config1.yaml\", \"r\") as file:\n",
    "    config = yaml.safe_load(file)\n",
    "    \n",
    "wandb_config = config[\"wandb\"]\n",
    "metrics_config = config[\"metrics\"]\n",
    "experiment_details = {'config': config}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user = os.environ['USER']\n",
    "slurm_jobid = os.getenv('SLURM_JOB_ID', None)\n",
    "scratch_dir = os.getenv('SCRATCH_DIR', '.')\n",
    "entity = os.getenv('WANDB_SEM_UNC_ENTITY', None)\n",
    "\n",
    "dir = f\"{scratch_dir}/{user}/{entity}\"\n",
    "if not os.path.exists(dir):\n",
    "        os.makedirs(dir)\n",
    "project = config[\"wandb\"][\"project\"]\n",
    "\n",
    "if config[\"wandb\"][\"debug\"]:\n",
    "    project = f\"{project}_debug\"\n",
    "\n",
    "experiment_lot = config[\"wandb\"]['experiment_lot']\n",
    "notes=f'slurm_id: {slurm_jobid}, experiment_lot: {experiment_lot}'\n",
    "\n",
    "wandb.init(\n",
    "    entity=entity,\n",
    "    project= project,\n",
    "    dir=dir,\n",
    "    config=config,\n",
    "    notes=notes,\n",
    ")\n",
    "\n",
    "logging.info('Finished wandb init.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_loader = Dataset(config)\n",
    "train_dataset, validation_dataset = dataset_loader.load_data()\n",
    "\n",
    "\n",
    "if not isinstance(train_dataset, list):\n",
    "        logging.info('Train dataset: %s', train_dataset)\n",
    "\n",
    "answerable_indices, unanswerable_indices = split_dataset(train_dataset)\n",
    "\n",
    "\n",
    "if config[\"dataset\"]['answerable_only']:\n",
    "        unanswerable_indices = []\n",
    "        val_answerable, val_unanswerable = split_dataset(validation_dataset)\n",
    "        del val_unanswerable\n",
    "        validation_dataset = [validation_dataset[i] for i in val_answerable]\n",
    "        train_dataset = [train_dataset[i] for i in answerable_indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The prompt is used in every sampling process.\n",
    "promptgenerator = PromptGenerator(config,train_dataset)\n",
    "few_shot_prompt,prompt_indices = promptgenerator.construct_fewshot_prompt_by_nums(2)\n",
    "experiment_details['prompt_indices'] = prompt_indices\n",
    "experiment_details['prompt'] = few_shot_prompt\n",
    "experiment_details['BRIEF'] = promptgenerator.BRIEF\n",
    "logging.info('Prompt is: %s', few_shot_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface_model = HuggingfaceModel(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## P_True Measure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = get_metric('squad')\n",
    "validation_promptgenerator = PromptGenerator(config,validation_dataset)\n",
    "p_true_evaluator = PTrueEvaluator(config,huggingface_model,promptgenerator,validation_promptgenerator,metric,experiment_details,results_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_true_few_shot_prompt = p_true_evaluator.construct_few_shot_prompt_for_p_true(few_shot_prompt,5,3)\n",
    "#wandb.config.update({'p_true_num_fewshot': len_p_true}, allow_val_change=True)\n",
    "#wandb.log(dict(len_p_true=len_p_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_answer_nums_each_question = 2\n",
    "t = 1.0 \n",
    "question_nums = 3\n",
    "p_true_evaluator.all_evaluate(few_shot_prompt,t,generate_answer_nums_each_question,p_true_few_shot_prompt,question_nums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'config': {'wandb': {'debug': False, 'project': 'test', 'experiment_lot': 'MyExperiment'}, 'dataset': {'name': 'squad', 'seed': 42, 'answerable_only': True}, 'prompt': {'few-shot': False, 'shot_num': 3, 'brief_always': True, 'use_context': True, 'add_tag': True, 'prompt_template_path': './data/prompt_templates/ask_templates/test2.txt'}, 'model': {'model_name': 'meta-llama/Llama-2-7b-hf', 'stop_sequences': 'default', 'max_new_tokens': 50}, 'sample': {'temperature': 1.0, 'sample_count': 5, 'sampling_method': 'simple_sample'}, 'metrics': [{'name': 'p_true', 'p_true_num_fewshot': 2}, {'name': 'accuracy'}, {'name': 'diversity'}], 'p_true': {'compute_p_true': True, 'get_training_set_generations': True, 'get_training_set_generations_most_likely_only': True, 'compute_accuracy_at_all_temps': True, 'p_true_hint': False}}, 'prompt_indices': [18303, 24501], 'prompt': 'Answer the following question as briefly as possible.\\nContext: Jean-Jacques Rousseau was the first of many to present the Alps as a place of allure and beauty, banishing the prevalent conception of the mountains as a hellish wasteland inhabited by demons. Rousseau\\'s conception of alpine purity was later emphasized with the publication of Albrecht von Haller\\'s poem Die Alpen that described the mountains as an area of mythical purity. Late in the 18th century the first wave of Romantics such as Goethe and Turner came to admire the scenery; Wordsworth visited the area in 1790, writing of his experiences in The Prelude. Schiller later wrote the play William Tell romanticising Swiss independence. After the end of the Napoleonic Wars, the Alpine countries began to see an influx of poets, artists, and musicians, as visitors came to experience the sublime effects of monumental nature.\\nQuestion: Who was the first of many to present the Alps as a place of allure and beauty?\\nAnswer: Jean-Jacques Rousseau\\n\\nAnswer the following question as briefly as possible.\\nContext: Grape juice is obtained from crushing and blending grapes into a liquid. The juice is often sold in stores or fermented and made into wine, brandy, or vinegar. Grape juice that has been pasteurized, removing any naturally occurring yeast, will not ferment if kept sterile, and thus contains no alcohol. In the wine industry, grape juice that contains 7–23% of pulp, skins, stems and seeds is often referred to as \"must\". In North America, the most common grape juice is purple and made from Concord grapes, while white grape juice is commonly made from Niagara grapes, both of which are varieties of native American grapes, a different species from European wine grapes. In California, Sultana (known there as Thompson Seedless) grapes are sometimes diverted from the raisin or table market to produce white juice.\\nQuestion: What juice is made when grapes are crushed and blended?\\nAnswer: Grape juice\\n\\n', 'BRIEF': 'Answer the following question as briefly as possible.\\n', 'p_true_indices': [12260, 34206, 77660], 'p_true_responses': {12260: {'responses': ['Sun Jiadong', 'Sun Jiadong', 'Sun Jiadong', 'Sun Jiadong', 'Sun Jiadong', 'Sun Jiadong'], 'most_likely_response': 'Sun Jiadong', 'is_correct': 1.0}, 34206: {'responses': ['661–80', '661', 'After Uthman ibn Affan', '661–80', '661–80', 'In 680'], 'most_likely_response': '661–80', 'is_correct': 0.0}, 77660: {'responses': ['Tales of Phantasia and Star Ocean', 'Tales of Phantasia and Star Ocean', 'Tales of Phantasia, Star Ocean', 'Tales of Phantasia and Star Ocean', 'Tales of Phantasia and Star Ocean', 'Tales of Phantasia and Star Ocean'], 'most_likely_response': 'Tales of Phantasia and Star Ocean', 'is_correct': 1.0}}, 'p_true_few_shot_prompt': 'Question: Who designed the COMPASS navigation system?\\nBrainstormed Answers: Sun Jiadong \\nSun Jiadong \\nSun Jiadong \\nSun Jiadong \\nSun Jiadong \\nSun Jiadong \\nPossible answer: Sun Jiadong\\nIs the possible answer:\\nA) True\\nB) False\\nThe possible answer is: A\\nQuestion: When did Muawiyah become caliph?\\nBrainstormed Answers: 661–80 \\n661 \\nAfter Uthman ibn Affan \\n661–80 \\n661–80 \\nIn 680 \\nPossible answer: 661–80\\nIs the possible answer:\\nA) True\\nB) False\\nThe possible answer is: B\\nQuestion: What were the largest SNES games?\\nBrainstormed Answers: Tales of Phantasia and Star Ocean \\nTales of Phantasia and Star Ocean \\nTales of Phantasia, Star Ocean \\nTales of Phantasia and Star Ocean \\nTales of Phantasia and Star Ocean \\nTales of Phantasia and Star Ocean \\nPossible answer: Tales of Phantasia and Star Ocean\\nIs the possible answer:\\nA) True\\nB) False\\nThe possible answer is: A', 'train': {'indices': [7762, 79399, 79530]}, 'validation': {'indices': [4591, 4302, 1609]}}\n"
     ]
    }
   ],
   "source": [
    "print(experiment_details)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P_ik"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-27 00:56:43 INFO     validation_is_true: 1.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-27 00:56:43 INFO     validation_is_true: 1.000000\n",
      "2024-11-27 00:56:43 INFO     validation_is_true: 1.000000\n"
     ]
    }
   ],
   "source": [
    "use_all_generations = True\n",
    "condition_on_question = False\n",
    "strict_entailment = True\n",
    "use_num_generations = 2\n",
    "validation_embeddings, validation_is_true, validation_answerable = [], [], []\n",
    "def is_answerable(generation):\n",
    "    return len(generation['reference']['answers']['text']) > 0\n",
    "\n",
    "import pickle\n",
    "\n",
    "file_path = \"/home/yw699/codes/LLM-halu/results/validation_generations.pkl\"\n",
    "\n",
    "with open(file_path, \"rb\") as f:  # \"rb\" 表示以二进制读模式打开\n",
    "    validation_generations = pickle.load(f)\n",
    "\n",
    "for idx, tid in enumerate(validation_generations):\n",
    "\n",
    "    example = validation_generations[tid]\n",
    "    question = example['question']\n",
    "    context = example['context']\n",
    "    full_responses = example[\"responses\"]\n",
    "    most_likely_answer = example['most_likely_answer']\n",
    "\n",
    "    if not use_all_generations:\n",
    "        if use_num_generations == -1:\n",
    "            raise ValueError\n",
    "        responses = [fr[0] for fr in full_responses[:use_num_generations]]\n",
    "    else:\n",
    "        responses = [fr[0] for fr in full_responses]\n",
    "\n",
    "    validation_is_true.append(most_likely_answer['accuracy'])\n",
    "    validation_answerable.append(is_answerable(example))\n",
    "    #validation_embeddings.append(most_likely_answer['embedding'])\n",
    "    logging.info('validation_is_true: %f', validation_is_true[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metrics import *\n",
    "from collections import defaultdict\n",
    "\n",
    "entropies = defaultdict(list)\n",
    "\n",
    "entailment_model = EntailmentDeberta()\n",
    "\n",
    "\n",
    "responses = [f'{question} {r}' for r in responses]\n",
    "\n",
    "\n",
    "# Token log likelihoods. Shape = (n_sample, n_tokens)\n",
    "if not use_all_generations:\n",
    "    log_liks = [r[1] for r in full_responses[:args.use_num_generations]]\n",
    "else:\n",
    "    log_liks = [r[1] for r in full_responses]\n",
    "\n",
    "for i in log_liks:\n",
    "    assert i\n",
    "\n",
    "\n",
    "# Compute context entails answer baseline.\n",
    "entropies['context_entails_response'].append(context_entails_response(context, responses, entailment_model))\n",
    "\n",
    "if condition_on_question and entailment_model == 'deberta':\n",
    "    responses = [f'{question} {r}' for r in responses]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cluster_assignment_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute semantic ids.\n",
    "semantic_ids = get_semantic_ids(responses, model=entailment_model,strict_entailment=strict_entailment, example=example)\n",
    "results_dict['semantic_ids'] = []\n",
    "results_dict['semantic_ids'].append(semantic_ids)\n",
    "\n",
    "# Compute entropy from frequencies of cluster assignments.\n",
    "entropies['cluster_assignment_entropy'].append(cluster_assignment_entropy(semantic_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length normalization of generation probabilities.\n",
    "log_liks_agg = [np.mean(log_lik) for log_lik in log_liks]\n",
    "\n",
    "# Compute naive entropy.\n",
    "entropies['regular_entropy'].append(predictive_entropy(log_liks_agg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-27 01:02:51 INFO     ################################################################################\n",
      "2024-11-27 01:02:51 INFO     NEW ITEM 2 at id=`57269cc3dd62a815002e8b13`.\n",
      "2024-11-27 01:02:51 INFO     Context:\n",
      "2024-11-27 01:02:51 INFO     While the Treaties and Regulations will have direct effect (if clear, unconditional and immediate), Directives do not generally give citizens (as opposed to the member state) standing to sue other citizens. In theory, this is because TFEU article 288 says Directives are addressed to the member states and usually \"leave to the national authorities the choice of form and methods\" to implement. In part this reflects that directives often create minimum standards, leaving member states to apply higher standards. For example, the Working Time Directive requires that every worker has at least 4 weeks paid holidays each year, but most member states require more than 28 days in national law. However, on the current position adopted by the Court of Justice, citizens have standing to make claims based on national laws that implement Directives, but not from Directives themselves. Directives do not have so called \"horizontal\" direct effect (i.e. between non-state parties). This view was instantly controversial, and in the early 1990s three Advocate Generals persuasively argued that Directives should create rights and duties for all citizens. The Court of Justice refused, but there are five large exceptions.\n",
      "2024-11-27 01:02:51 INFO     Question:\n",
      "2024-11-27 01:02:51 INFO     How many paid holiday days does the Working Time directive require workers to have each year?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-27 01:02:51 INFO     True Answers:\n",
      "2024-11-27 01:02:51 INFO     {'answers': {'answer_start': [594, 594, 594], 'text': ['4 weeks', '4 weeks paid holidays each year', '4 weeks paid']}, 'id': '57269cc3dd62a815002e8b13'}\n",
      "2024-11-27 01:02:51 INFO     Low Temperature Generation:\n",
      "2024-11-27 01:02:51 INFO     4 weeks\n",
      "2024-11-27 01:02:51 INFO     Low Temperature Generation Accuracy:\n",
      "2024-11-27 01:02:51 INFO     1.0\n",
      "2024-11-27 01:02:51 INFO     High Temp Generation:\n",
      "2024-11-27 01:02:51 INFO     ['4 weeks', '4 weeks']\n",
      "2024-11-27 01:02:51 INFO     High Temp Generation:\n",
      "2024-11-27 01:02:51 INFO     semantic_ids: [0, 0], avg_token_log_likelihoods: [0.0, 0.0], entropies: context_entails_response:1.00, cluster_assignment_entropy:-0.00, regular_entropy:-0.00, semantic_entropy:-0.00\n"
     ]
    }
   ],
   "source": [
    "# Compute semantic entropy.\n",
    "log_likelihood_per_semantic_id = logsumexp_by_id(semantic_ids, log_liks_agg, agg='sum_normalized')\n",
    "pe = predictive_entropy_rao(log_likelihood_per_semantic_id)\n",
    "entropies['semantic_entropy'].append(pe)\n",
    "\n",
    "results_dict['uncertainty_measures'].update(entropies)\n",
    "\n",
    "\n",
    "\n",
    "log_str = 'semantic_ids: %s, avg_token_log_likelihoods: %s, entropies: %s'\n",
    "entropies_fmt = ', '.join([f'{i}:{j[-1]:.2f}' for i, j in entropies.items()])\n",
    "# pylint: enable=invalid-name\n",
    "logging.info(80*'#')\n",
    "logging.info('NEW ITEM %d at id=`%s`.', idx, tid)\n",
    "logging.info('Context:')\n",
    "logging.info(example['context'])\n",
    "logging.info('Question:')\n",
    "logging.info(question)\n",
    "logging.info('True Answers:')\n",
    "logging.info(example['reference'])\n",
    "logging.info('Low Temperature Generation:')\n",
    "logging.info(most_likely_answer['response'])\n",
    "logging.info('Low Temperature Generation Accuracy:')\n",
    "logging.info(most_likely_answer['accuracy'])\n",
    "logging.info('High Temp Generation:')\n",
    "logging.info([r[0] for r in full_responses])\n",
    "logging.info('High Temp Generation:')\n",
    "logging.info(log_str, semantic_ids, log_liks_agg, entropies_fmt)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic_uncertainty2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输入results_old\n",
    "\n",
    "'''\n",
    "results_old['validation_is_false']\n",
    "results_old['uncertainty_measures']\n",
    "results_old['uncertainty_measures']['p_false']\n",
    "results_old['uncertainty_measures']['p_false_fixed']\n",
    "results_old['validation_unanswerable']\n",
    "\n",
    "\n",
    "#### for measure_name, measure_values in rum.items():\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for measure_name, measure_values in rum.items():\n",
    "        logging.info('Computing for uncertainty measure `%s`.', measure_name)\n",
    "\n",
    "        # Validation accuracy.\n",
    "        validation_is_falses = [\n",
    "            results_old['validation_is_false'],\n",
    "            results_old['validation_unanswerable']\n",
    "        ]\n",
    "\n",
    "        logging_names = ['', '_UNANSWERABLE']\n",
    "\n",
    "        # Iterate over predictions of 'falseness' or 'answerability'.\n",
    "        # 指标计算\n",
    "        for validation_is_false, logging_name in zip(validation_is_falses, logging_names):\n",
    "            name = measure_name + logging_name\n",
    "            result_dict['uncertainty'][name] = {}\n",
    "\n",
    "            validation_is_false = np.array(validation_is_false)\n",
    "            validation_accuracy = 1 - validation_is_false\n",
    "            if len(measure_values) > len(validation_is_false):\n",
    "                # This can happen, but only for p_false.\n",
    "                if 'p_false' not in measure_name:\n",
    "                    raise ValueError\n",
    "                logging.warning(\n",
    "                    'More measure values for %s than in validation_is_false. Len(measure values): %d, Len(validation_is_false): %d',\n",
    "                    measure_name, len(measure_values), len(validation_is_false))\n",
    "                measure_values = measure_values[:len(validation_is_false)]\n",
    "\n",
    "            # fargs = {\n",
    "            #     'AUROC': [validation_is_false, measure_values],\n",
    "            #     'area_under_thresholded_accuracy': [validation_accuracy, measure_values],\n",
    "            #     'mean_uncertainty': [measure_values]}\n",
    "\n",
    "            # for answer_fraction in answer_fractions:\n",
    "            #     fargs[f'accuracy_at_{answer_fraction}_answer_fraction'] = [validation_accuracy, measure_values]\n",
    "\n",
    "            # for fname, (function, bs_function) in eval_metrics.items():\n",
    "            #     metric_i = function(*fargs[fname])\n",
    "            #     result_dict['uncertainty'][name][fname] = {}\n",
    "            #     result_dict['uncertainty'][name][fname]['mean'] = metric_i\n",
    "            #     logging.info(\"%s for measure name `%s`: %f\", fname, name, metric_i)\n",
    "            #     result_dict['uncertainty'][name][fname]['bootstrap'] = bs_function(\n",
    "            #         function, rng)(*fargs[fname])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 将结果都存放在 result_dict 中\n",
    "result_dict = {'performance': {}, 'uncertainty': {}}\n",
    "\n",
    "## 测量 performance\n",
    "all_accuracies['accuracy'] = 1 - np.array(results_old['validation_is_false'])\n",
    "\n",
    "for name, target in all_accuracies.items():\n",
    "    result_dict['performance'][name] = {}\n",
    "    result_dict['performance'][name]['mean'] = np.mean(target)\n",
    "    result_dict['performance'][name]['bootstrap'] = bootstrap(np.mean, rng)(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 修改p_true\n",
    "##转换p_true为概率值\n",
    "rum = results_old['uncertainty_measures']\n",
    "if 'p_false' in rum and 'p_false_fixed' not in rum:\n",
    "    # Restore log probs true: y = 1 - x --> x = 1 - y.\n",
    "    # Convert to probs --> np.exp(1 - y).\n",
    "    # Convert to p_false --> 1 - np.exp(1 - y).\n",
    "    rum['p_false_fixed'] = [1 - np.exp(1 - x) for x in rum['p_false']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "val_metrics 是一个字典，键是指标名称（如 'AUROC'），值是一个元组 (function, bs_function)\n",
    "function 用于计算该指标的函数。\n",
    "bs_function：用于对该指标进行 bootstrap 分析 的函数\n",
    "\n",
    "\n",
    "\n",
    "eval_metrics = {\n",
    "    'AUROC': (auroc, compatible_bootstrap),\n",
    "    'area_under_thresholded_accuracy': (area_under_thresholded_accuracy, compatible_bootstrap),\n",
    "    'mean_uncertainty': (np.mean, bootstrap)\n",
    "    }\n",
    "\n",
    "'''\n",
    "\n",
    " eval_metrics = dict(zip(\n",
    "        ['AUROC', 'area_under_thresholded_accuracy', 'mean_uncertainty'],\n",
    "        list(zip([auroc, area_under_thresholded_accuracy, np.mean],[compatible_bootstrap, compatible_bootstrap, bootstrap])),\n",
    "    ))\n",
    "\n",
    "for answer_fraction in answer_fractions:\n",
    "        key = f'accuracy_at_{answer_fraction}_answer_fraction'\n",
    "        eval_metrics[key] = [functools.partial(accuracy_at_quantile, quantile=answer_fraction),compatible_bootstrap]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for measure_name, measure_values in rum.items():\n",
    "        logging.info('Computing for uncertainty measure `%s`.', measure_name)\n",
    "\n",
    "        # Validation accuracy.\n",
    "        validation_is_falses = [\n",
    "            results_old['validation_is_false'],\n",
    "            results_old['validation_unanswerable']\n",
    "        ]\n",
    "\n",
    "        logging_names = ['', '_UNANSWERABLE']\n",
    "\n",
    "        # Iterate over predictions of 'falseness' or 'answerability'.\n",
    "        # 指标计算\n",
    "        for validation_is_false, logging_name in zip(validation_is_falses, logging_names):\n",
    "            name = measure_name + logging_name\n",
    "            result_dict['uncertainty'][name] = {}\n",
    "\n",
    "            validation_is_false = np.array(validation_is_false)\n",
    "            validation_accuracy = 1 - validation_is_false\n",
    "            if len(measure_values) > len(validation_is_false):\n",
    "                # This can happen, but only for p_false.\n",
    "                if 'p_false' not in measure_name:\n",
    "                    raise ValueError\n",
    "                logging.warning(\n",
    "                    'More measure values for %s than in validation_is_false. Len(measure values): %d, Len(validation_is_false): %d',\n",
    "                    measure_name, len(measure_values), len(validation_is_false))\n",
    "                measure_values = measure_values[:len(validation_is_false)]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                # fargs 字典：评估任务 + 所需要的指标\n",
    "                # 实验指标通过fargs字典与eval_metrics字典实现可拓展\n",
    "                # 汇总每个评估指标以及所需要的不同输入数据\n",
    "                fargs = {\n",
    "                        'AUROC': [validation_is_false, measure_values],\n",
    "                        'area_under_thresholded_accuracy': [validation_accuracy, measure_values],\n",
    "                        'mean_uncertainty': [measure_values]}\n",
    "\n",
    "                #添加新的评估任务\n",
    "                for answer_fraction in answer_fractions:\n",
    "                    fargs[f'accuracy_at_{answer_fraction}_answer_fraction'] = [validation_accuracy, measure_values]\n",
    "\n",
    "                '''\n",
    "                fargs = {\n",
    "                    'AUROC': [validation_is_false, measure_values],\n",
    "                    'area_under_thresholded_accuracy': [validation_accuracy, measure_values],\n",
    "                    'mean_uncertainty': [measure_values],\n",
    "                    'accuracy_at_0.5_answer_fraction': [validation_accuracy, measure_values],\n",
    "                    'accuracy_at_0.75_answer_fraction': [validation_accuracy, measure_values]\n",
    "                }\n",
    "                '''\n",
    "\n",
    "\n",
    "                for fname, (function, bs_function) in eval_metrics.items():\n",
    "                    metric_i = function(*fargs[fname])\n",
    "                    result_dict['uncertainty'][name][fname] = {}\n",
    "                    result_dict['uncertainty'][name][fname]['mean'] = metric_i\n",
    "                    logging.info(\"%s for measure name `%s`: %f\", fname, name, metric_i)\n",
    "                    result_dict['uncertainty'][name][fname]['bootstrap'] = bs_function(\n",
    "                        function, rng)(*fargs[fname])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##最终的result_dict\n",
    "\n",
    "'''\n",
    "result_dict = {\n",
    "    'performance': {},  # 存储性能指标的结果\n",
    "    'uncertainty': {}   # 存储不确定性测量的结果\n",
    "}\n",
    "\n",
    "'performance': {\n",
    "    'accuracy': {  # 性能指标名称\n",
    "        'mean': 0.95,  # 准确率的均值\n",
    "        'bootstrap': [0.94, 0.96]  # 使用 bootstrap 计算的置信区间\n",
    "    }\n",
    "}\n",
    "\n",
    "'uncertainty': {\n",
    "    'p_false': {  # 不确定性测量名称（如 p_false）\n",
    "        'AUROC': {\n",
    "            'mean': 0.85,  # AUROC 的均值\n",
    "            'bootstrap': [0.83, 0.87]  # AUROC 的置信区间\n",
    "        },\n",
    "        'mean_uncertainty': {\n",
    "            'mean': 0.3,  # 不确定性测量的均值\n",
    "            'bootstrap': [0.28, 0.32]  # 不确定性测量的置信区间\n",
    "        },\n",
    "        'accuracy_at_0.8_answer_fraction': {\n",
    "            'mean': 0.88,  \n",
    "            'bootstrap': [0.86, 0.89]\n",
    "        },\n",
    "        ...\n",
    "    },\n",
    "    'p_false_UNANSWERABLE': {  # 对应另一个验证指标的结果\n",
    "        'AUROC': {\n",
    "            'mean': 0.75,\n",
    "            'bootstrap': [0.73, 0.77]\n",
    "        },\n",
    "        ...\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare: performance of this uncertain metrics\n",
    "Through the value of auroc, we compare these methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import wandb\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_metrics = {\n",
    "    \"uncertainty\": {\n",
    "        \"semantic_entropy\": {\n",
    "            \"AUROC\": {\"mean\": 0.75},\n",
    "            \"Accuracy\": {\"mean\": 0.85}\n",
    "        },\n",
    "        \"cluster_assignment_entropy\": {\n",
    "            \"AUROC\": {\"mean\": 0.72},\n",
    "            \"Accuracy\": {\"mean\": 0.83}\n",
    "        },\n",
    "        \"regular_entropy\": {\n",
    "            \"AUROC\": {\"mean\": 0.70},\n",
    "            \"Accuracy\": {\"mean\": 0.80}\n",
    "        },\n",
    "        \"p_false\": {\n",
    "            \"AUROC\": {\"mean\": 0.68},\n",
    "            \"Accuracy\": {\"mean\": 0.78}\n",
    "        },\n",
    "        \"p_ik\": {\n",
    "            \"AUROC\": {\"mean\": 0.74},\n",
    "            \"Accuracy\": {\"mean\": 0.82}\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb_id = 'h1scz5qz'\n",
    "# if wandb_id == 'YOUR_ID':\n",
    "#     raise ValueError('Need to provide wandb_id of demo run!')\n",
    "# def restore_file(wandb_id, filename='wandb-summary.json'):\n",
    "#     files_dir = 'notebooks/restored_files'    \n",
    "#     os.system(f'mkdir -p {files_dir}')\n",
    "\n",
    "#     api = wandb.Api()\n",
    "#     run = api.run(f'semantic_uncertainty/{wandb_id}')\n",
    "\n",
    "#     path = f'{files_dir}/{filename}'\n",
    "#     os.system(f'rm -rf {path}')\n",
    "#     run.file(filename).download(root=files_dir, replace=True, exist_ok=False)\n",
    "#     with open(path, 'r') as f:\n",
    "#         out = json.load(f)\n",
    "#     return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uncertainty_df(metrics):\n",
    "    data = []\n",
    "    for method in metrics['uncertainty']:\n",
    "        for metric in metrics['uncertainty'][method]:\n",
    "            mean = metrics['uncertainty'][method][metric]['mean']\n",
    "            data.append([method, metric, mean])\n",
    "    df = pd.DataFrame(data, columns=['method', 'metric', 'means'])\n",
    "    main_methods = ['semantic_entropy', 'cluster_assignment_entropy', 'regular_entropy', 'p_false', 'p_ik']\n",
    "    df = df.set_index('method').loc[main_methods].reset_index()\n",
    "    main_names = ['Semantic entropy', 'Discrete Semantic Entropy', 'Naive Entropy', 'p(True)', 'Embedding Regression']\n",
    "    conversion = dict(zip(main_methods, main_names))\n",
    "    df['method'] = df.method.map(lambda x: conversion[x])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unc_df = get_uncertainty_df(example_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = 'AUROC'\n",
    "unc_df.set_index('metric').loc[metric].plot.bar(x='method', y='means')\n",
    "plt.gca().set_ylabel(metric)\n",
    "plt.gca().grid(axis='y')\n",
    "plt.gca().set_ylim(0.6, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = 'Accuracy'\n",
    "unc_df.set_index('metric').loc[metric].plot.bar(x='method', y='means')\n",
    "plt.gca().set_ylabel(metric)\n",
    "plt.gca().grid(axis='y')\n",
    "plt.gca().set_ylim(0.6, 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "semantic_uncertainty",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
