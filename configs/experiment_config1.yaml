experiment:

  root_dir: "/home/yw699/codes/LLM-Hallu/"
  # "squad"/'svamp'/'nq'/"trivia_qa"/'trustful_qa_mc1'/'trustful_qa_mc2'
  dataset: 'trustful_mc2'
  seed: 42


  #few-shot setting
  few-shot: False
  shot_num: 3
  brief_always: True

  add_tag: True
  prompt_template_path: "/home/yw699/codes/LLM-Hallu/data/prompt_templates/ask_templates/test2.txt"
  

  model_name: "meta-llama/Llama-2-7b-hf"
  
  stop_sequences: "default"     # None/"default"
  max_new_tokens: 50
  
  







  temperature: 1.0
  sample_count: 5
  sampling_method: "simple_sample"    # "simple_sample"/"adv_prompt_sample"/"similar_prompt_sample"/"process_change_sample"
  
  
 
  metrics:
    - "accuracy"
    - "diversity"