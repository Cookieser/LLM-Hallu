# README

This is a project to summary the existed ground truth and methods to detect the hallucination in large language model.

We will list and briefly introduct all related articles here and consturct the codes to summary all methods in one system.

We will try running different measurement methods and then integrate them to provide a unified interface for subsequent testing.


## Ground Truth/confidence
- Semantic Entropy

https://arxiv.org/abs/2302.09664
https://www.nature.com/articles/s41586-024-07421-0

Use the Semantic Entropy to measure the confidence of llm output


## Metics

- TrustfulQA
    - MC1/MC2:  multiple choice to measure the truth
    - Open questions: Use judge like GPT-4




